{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic Text-Preprocessing, Sentiment Analysis using Naive Bayes.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanshikapandey/NLP-Basics/blob/main/Basic_Text_Preprocessing%2C_Sentiment_Analysis_using_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGxGF7q5C5_F"
      },
      "source": [
        "**INSTALLING NLTK AND DOWNLOADING THE DATA**\n",
        "\n",
        "In this step you will install NLTK and download the sample tweets (or any other dataset) that you will use to train and test your model.\n",
        "This will import three datasets from NLTK that contain various tweets to train and test the model:\n",
        "\n",
        "negative_tweets.json: 5000 tweets with negative sentiments\n",
        "\n",
        "positive_tweets.json: 5000 tweets with positive sentiments\n",
        "\n",
        "tweets.20150430-223406.json: 20000 tweets with no sentiments\n",
        "\n",
        "NLTK Documentation can be found here: https://www.nltk.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz5hDOZ8CX5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08898232-e090-4af0-f907-272b570edd21"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.tag import pos_tag\n",
        "import re, string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlYvFKsGg-Dp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6cd146-c8ef-4b68-c97a-3fdb10b04bd7"
      },
      "source": [
        "#Creating Variables for +ve, -ve and text(neutral) tweets\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
        "for tweet in positive_tweets[:5]:\n",
        "\tprint (tweet)\n",
        "#type(positive_tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
            "@97sides CONGRATS :)\n",
            "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krjmsUCkDf2h",
        "outputId": "3946492d-a89f-4321-c69b-0f7fbf94778e"
      },
      "source": [
        "for tweet in negative_tweets[:5]:\n",
        "\tprint (tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hopeless for tmr :(\n",
            "Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
            "@Hegelbon That heart sliding into the waste basket. :(\n",
            "“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\n",
            "\n",
            "Me too\n",
            "Dang starting next week I have \"work\" :(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_im6hL7DoJz",
        "outputId": "884e78bb-271e-4476-a18d-3958faaa8802"
      },
      "source": [
        "for tweet in text[:5]:\n",
        "\tprint (tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RT @KirkKus: Indirect cost of the UK being in the EU is estimated to be costing Britain £170 billion per year! #BetterOffOut #UKIP\n",
            "VIDEO: Sturgeon on post-election deals http://t.co/BTJwrpbmOY\n",
            "RT @LabourEoin: The economy was growing 3 times faster on the day David Cameron became Prime Minister than it is today.. #BBCqt http://t.co…\n",
            "RT @GregLauder: the UKIP east lothian candidate looks about 16 and still has an msn addy http://t.co/7eIU0c5Fm1\n",
            "RT @thesundaypeople: UKIP's housing spokesman rakes in £800k in housing benefit from migrants.  http://t.co/GVwb9Rcb4w http://t.co/c1AZxcLh…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpdXoPrGe2YA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d268d41d-227d-41d8-bd05-3bff690ece0f"
      },
      "source": [
        "#Tokenizing tweets\n",
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "#for i in tweet_tokens:\n",
        "#  print(i)\n",
        "print(tweet_tokens[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)'], ['@Lamb2ja', 'Hey', 'James', '!', 'How', 'odd', ':/', 'Please', 'call', 'our', 'Contact', 'Centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'Many', 'thanks', '!'], ['@DespiteOfficial', 'we', 'had', 'a', 'listen', 'last', 'night', ':)', 'As', 'You', 'Bleed', 'is', 'an', 'amazing', 'track', '.', 'When', 'are', 'you', 'in', 'Scotland', '?', '!'], ['@97sides', 'CONGRATS', ':)'], ['yeaaaah', 'yippppy', '!', '!', '!', 'my', 'accnt', 'verified', 'rqst', 'has', 'succeed', 'got', 'a', 'blue', 'tick', 'mark', 'on', 'my', 'fb', 'profile', ':)', 'in', '15', 'days']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN5cppVKiiG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "847c45a6-b074-4e35-f7fb-a4e88db61cf1"
      },
      "source": [
        "#POS Tagging\n",
        "\n",
        "print(pos_tag(tweet_tokens[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMx6vggIkGYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014330cd-0da5-4c37-839f-c790960b5883"
      },
      "source": [
        "#Lemmatization\n",
        "\n",
        "def lemmatize_sentence(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "    return lemmatized_sentence\n",
        "\n",
        "print(lemmatize_sentence(tweet_tokens[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg6Wiy4RkrJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322ebb27-209c-44ef-f5c3-71a1a46fa676"
      },
      "source": [
        "#Using Regular Expressions to remove NOISE (i.e. Stop-Words, Hyperlinks, Punctuations, URL, Twitter Handles [@-stuff])\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "def remove_noise(tweet_tokens):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://([a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(%[0-9a-fA-F][0-9a-fA-F]))+','', token)                 #REMOVING URLs\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)                    #REMOVING Twitter Handles\n",
        "\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)                        #Lemmatization\n",
        "       \n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:    #REMOVING Punctuations and Stop-Words\n",
        "            cleaned_tokens.append(token.lower())                                        #Converting to Lower-Case\n",
        "    return cleaned_tokens\n",
        "\n",
        "print(remove_noise(tweet_tokens[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPTIqIKbt45T"
      },
      "source": [
        "#Doing the above Data Cleaning for the entire dataset\n",
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []\n",
        "\n",
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens))\n",
        "\n",
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2UroybjvxDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c0dd8a-ac67-4676-a565-f01b644ecec2"
      },
      "source": [
        "print(positive_tweet_tokens[0])\n",
        "print(positive_cleaned_tokens_list[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
            "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aC4In9Qwlgl"
      },
      "source": [
        "#Preparing Data for the Model\n",
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)                    \n",
        "\n",
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
        "\n",
        "#type(positive_tokens_for_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUyhNE-lxJHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c04189-6a68-49ce-e4ed-9b7703b5a9c2"
      },
      "source": [
        "#Splitting the dataset for Testing and Training\n",
        "import random\n",
        "\n",
        "positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                     for tweet_dict in positive_tokens_for_model]\n",
        "\n",
        "negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                     for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]\n",
        "type(positive_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozNEbfdIxSbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d1a457-6b19-4fde-dc9b-8728184b523f"
      },
      "source": [
        "#Building and Testing the Model\n",
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "\n",
        "print(classifier.show_most_informative_features(10))                     #https://www.nltk.org/_modules/nltk/classify/naivebayes.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.997\n",
            "Most Informative Features\n",
            "                      :) = True           Positi : Negati =    986.9 : 1.0\n",
            "                     sad = True           Negati : Positi =     30.3 : 1.0\n",
            "                    glad = True           Positi : Negati =     23.0 : 1.0\n",
            "                follower = True           Positi : Negati =     22.6 : 1.0\n",
            "                     x15 = True           Negati : Positi =     19.0 : 1.0\n",
            "               community = True           Positi : Negati =     15.6 : 1.0\n",
            "                  arrive = True           Positi : Negati =     13.8 : 1.0\n",
            "                     via = True           Positi : Negati =     11.2 : 1.0\n",
            "                    kill = True           Negati : Positi =     10.4 : 1.0\n",
            "           unfortunately = True           Negati : Positi =     10.4 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biF02652xulo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80fef3ba-0647-441d-a59b-9393342b5738"
      },
      "source": [
        "# Testing\n",
        "\n",
        "custom_tweet = \"I welcomed them with a sad heart\"\n",
        "#custom_tweet = \"hello\"\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rcg6OgRx-1R"
      },
      "source": [
        "Reference: https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk#:~:text=Sentiment%20analysis%20is%20a%20common,Python%2C%20to%20analyze%20textual%20data."
      ]
    }
  ]
}